from __future__ import division
from __future__ import print_function

import numpy as np
import os
import shutil
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

from functools import partial
from pycpd import DeformableRegistration
import time

from FPS import FarthestSampler
import sub_module
from GCN_Learning import GCN_train
from Refinement import renew_label


def sort_by_number(file_name):
    Initial = int(file_name.split('_')[1])
    return Initial

# If standard_mode is True, implement the 3D-OGT(3D-NOD); otherwise, implement the 3D-OGT(by human).
standard_mode = False
home_path = os.path.dirname(os.getcwd())

if standard_mode:
    path = os.path.join(home_path, "instance_old_organ(3D-NOD)")
    ins_new_organ = os.path.join(home_path, "instance_new_organ(3D-NOD)")
else:
    path = os.path.join(home_path, "instance_old_organ(by human)")
    ins_new_organ = os.path.join(home_path, "instance_new_organ(by human)")

Filelist = sorted(os.listdir(path), key=sort_by_number)
new_Filelist = sorted(os.listdir(ins_new_organ), key=sort_by_number)
file_numbers = len(Filelist)

# The generated files are stored in the following folders
if standard_mode:
    output_folder = os.path.join(home_path, "output(3D-NOD)")
else:
    output_folder = os.path.join(home_path, "output(by_human)")
CPD_folder = os.path.join(output_folder, "CPD_folder")          # Stores the files generated by the last iteration of the CPD
graph_path = os.path.join(output_folder, "GCN_output")
save_path = os.path.join(output_folder, "predict_result")       # Stores the final prediction results

if not os.path.exists(output_folder):
    os.mkdir(output_folder)
if not os.path.exists(CPD_folder):
    os.mkdir(CPD_folder)
if not os.path.exists(graph_path):
    os.mkdir(graph_path)
if not os.path.exists(save_path):
    os.mkdir(save_path)

def main():
    FPS = FarthestSampler()
    sample_count = 500
    judge_start = 0
    sequence_num = 0
    current_max_label = 0
    if standard_mode:
        new_t_labels = []

    for i in range(file_numbers-1):
        source_file_norm = Filelist[i]
        target_file_norm = Filelist[i+1]
        parts_to_check = [source_file_norm.split('_')[2]==target_file_norm.split('_')[2],
                          source_file_norm.split('_')[3]==target_file_norm.split('_')[3],
                          source_file_norm.split('_')[4]==target_file_norm.split('_')[4]]

        if i == 0 or (False in parts_to_check):
            if i == 0:
                source_file_norm = Filelist[i]
                target_file_norm = Filelist[i+1]
            else:
                source_file_norm = Filelist[i+1]
                target_file_norm = Filelist[i+2]
                judge_start = sequence_num + 1
                current_max_label = 0

            shutil.copy(os.path.join(path, source_file_norm), save_path)
            source_points = np.loadtxt(os.path.join(save_path, source_file_norm))
            sample_points, uni_label = sub_module.Inhomogeneous_sampling(sample_count, source_points, FPS)        # 3.3 Inhomogeneous down-sampling

            max_label = np.max(np.unique(source_points[:, 3]))
            if max_label > current_max_label:
                current_max_label = max_label

            if standard_mode:
                source_lab, source_count = np.unique(source_points[:, 3], return_counts=True)
                new_t_labels = source_lab[source_count < 50].tolist()           # organs with fewer than 50 points are directly considered as new organs
                # the presence of new organs can't be determined at the initial moment of the sequence

        else:
            if judge_start != sequence_num:
                sequence_num += 1
                continue
            else:
                source_points = np.loadtxt(os.path.join(save_path, source_file_norm))      # Read files from output_folder
                sample_points, uni_label = sub_module.Inhomogeneous_sampling(sample_count, source_points, FPS)

        target_array = np.loadtxt(os.path.join(path, target_file_norm))
        sample_target_points = FPS._call__(target_array, sample_count)

        Backward_Registration = True
        M, N, all_label = sub_module.ICP_Registration_main(sample_points, sample_target_points, sample_count, Backward_Registration)    # Backward ICP Registration

        X, Y = M[:, :3], N[:, :3]
        callback_backward = partial(sub_module.visualize_backward, label=all_label,
                                    source_file_norm=source_file_norm, target_file_norm= target_file_norm, CPD_folder=CPD_folder)
        reg_backward = DeformableRegistration(**{'X': X, 'Y': Y, 'low_rank': False})                                                    # Deformable Registration
        reg_backward.register(callback_backward)
        file_name = "_".join(source_file_norm.split("_")[:-1]) + " & " + target_file_norm.split("_")[-2] + ".txt"
        Reg_data = np.loadtxt(CPD_folder + '\\' + file_name)

        sub_module.create_input(Reg_data, sample_count, graph_path)                                              # Create input for GCN learning
        Backward_result = GCN_train(sample_target_points[:, :3], sample_count, graph_path)                       # 3.4.2 GCN Learning

        Backward_Registration = False
        M, N, all_label = sub_module.ICP_Registration_main(sample_points, sample_target_points, sample_count, Backward_Registration)    # Forward ICP Registration

        X, Y = M[:, :3], N[:, :3]
        callback_forward = partial(sub_module.visualize_forward, label=all_label,
                                   source_file_norm=source_file_norm, target_file_norm= target_file_norm, CPD_folder=CPD_folder)
        reg_forward = DeformableRegistration(**{'X': Y, 'Y': X, 'low_rank': False})                                                     # Deformable Registration
        reg_forward.register(callback_forward)
        Reg_data = np.loadtxt(CPD_folder + '\\' + file_name)

        sub_module.create_input(Reg_data, sample_count, graph_path)                                               # Create input for GCN learning
        Forward_result = GCN_train(sample_target_points[:, :3], sample_count, graph_path)                         # 3.4.2 GCN Learning

        # 3.4.3.1 Intersection operation
        first_array, second_array = sub_module.Intersection(sample_count, Backward_result, Forward_result)

        # 3.4.3.2 Filtering operation
        result_array = sub_module.Filtering(first_array, second_array, sample_points, sample_count, Backward_result, Forward_result, FPS)

        target_data = np.vstack((result_array, target_array))
        sub_module.create_input(target_data, sample_count, graph_path)      # Create input for label Propagation
        train_result = GCN_train(None, sample_count, graph_path)            # 3.5 GCN Learning-based Propagation
        if len(train_result) == 2048:
            updated_labels, average_distance = renew_label(source_points, train_result)         # 3.5 Refinement operation
            train_result[:, 3] = updated_labels
            if standard_mode:
                new_t_labels = []                 # there is no new organ in current moment
            np.savetxt(save_path + '\\' + target_file_norm, train_result, delimiter=" ", fmt="%.6f %.6f %.6f %d")
        else:
            updated_labels, average_distance = renew_label(source_points, train_result)         # 3.5 Refinement operation
            new_coordinates = np.loadtxt(os.path.join(ins_new_organ, target_file_norm))
            new_uni_label = np.unique(new_coordinates[:, 3])

            if standard_mode:
                used_t_labels = set()
                for new_label in new_uni_label:
                    # In 3D-NOD module, duplicate detection of new organs may still occur due to the presence of BFL, eliminating the effect of duplicate detection
                    merged = sub_module.eliminate_BFL(new_coordinates, train_result, source_points, new_label, new_t_labels, used_t_labels, average_distance)
                    if not merged:
                        current_max_label += 1
                        new_coordinates[new_coordinates[:, 3] == new_label, 3] = current_max_label
            else:
                for new_label in new_uni_label:
                    current_max_label += 1
                    new_coordinates[new_coordinates[:, 3] == new_label, 3] = current_max_label
            train_result[:, 3] = updated_labels
            train_result = np.vstack((new_coordinates, train_result))
            if standard_mode:
                new_t_labels = np.unique(train_result[train_result[:, 3] > np.max(uni_label), 3]).tolist()
            np.savetxt(save_path + '\\' + target_file_norm, train_result, delimiter=" ", fmt="%.6f %.6f %.6f %d")


if __name__ == '__main__':
    first_time = time.time()
    main()
    print(f'run.time:{time.time() - first_time:.4f}s')